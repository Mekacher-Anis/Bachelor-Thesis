
@inproceedings{riaz_fouriernet_2021,
	title = {{FourierNet}: Compact Mask Representation for Instance Segmentation Using Differentiable Shape Decoders},
	doi = {10.1109/ICPR48806.2021.9413048},
	shorttitle = {{FourierNet}},
	abstract = {We present {FourierNet}, a single shot, anchor-free, fully convolutional instance segmentation method that predicts a shape vector. Consequently, this shape vector is converted into the masks' contour points using a fast numerical transform. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which manages the automatic weight balancing of the shape vector's coefficients. We used the Fourier series as a shape encoder because of its coefficient interpretability and fast implementation. {FourierNet} shows promising results compared to polygon representation methods, achieving 30.6 {mAP} on the {MS} {COCO} 2017 benchmark. At lower image resolutions, it runs at 26.6 {FPS} with 24.3 {mAP}. It reaches 23.3 {mAP} using just eight parameters to represent the mask (note that at least four parameters are needed for bounding box prediction only). Qualitative analysis shows that suppressing a reasonable proportion of higher frequencies of Fourier series, still generates meaningful masks. These results validate our understanding that lower frequency components hold higher information for the segmentation task, and therefore, we can achieve a compressed representation. Code is available at: github.com/cogsys-tuebingen/{FourierNet}.},
	eventtitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	pages = {7833--7840},
	booktitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	author = {Riaz, Hamd Ul Moqeet and Benbarka, Nuri and Zell, Andreas},
	date = {2021-01},
	note = {{ISSN}: 1051-4651},
	keywords = {Decoding, Detectors, Fourier series, Image resolution, Instance segmentation, Real-time systems, Shape, Training, Transforms, anchor-free, differentiable algorithms, shape encoding},
}

@inproceedings{wang_shape_2019,
	title = {Shape Robust Text Detection With Progressive Scale Expansion Network},
	doi = {10.1109/CVPR.2019.00956},
	abstract = {Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network ({PSENet}), which can precisely detect text instances with arbitrary shapes. More specifically, {PSENet} generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on {CTW}1500, Total-Text, {ICDAR} 2015 and {ICDAR} 2017 {MLT} validate the effectiveness of {PSENet}. Notably, on {CTW}1500, a dataset full of long curve texts, {PSENet} achieves a F-measure of 74.3\% at 27 {FPS}, and our best F-measure (82.2\%) outperforms state-of-art algorithms by 6.6\%. The code will be released in the future.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {9328--9337},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai},
	date = {2019-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Categorization, Deep Learning, Recognition: Detection, Retrieval},
}

@misc{liao_real-time_2019,
	title = {Real-time Scene Text Detection with Differentiable Binarization},
	url = {http://arxiv.org/abs/1911.08947},
	doi = {10.48550/arXiv.1911.08947},
	abstract = {Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization ({DB}), which can perform the binarization process in a segmentation network. Optimized along with a {DB} module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of {DB} on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by {DB} are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of {ResNet}-18, our detector achieves an F-measure of 82.8, running at 62 {FPS}, on the {MSRA}-{TD}500 dataset. Code is available at: https://github.com/{MhLiao}/{DB}},
	publisher = {{arXiv}},
	author = {Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},
	urldate = {2023-01-17},
	date = {2019-12-03},
	eprinttype = {arxiv},
	eprint = {1911.08947 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liao_real-time_2023,
	title = {Real-Time Scene Text Detection With Differentiable Binarization and Adaptive Scale Fusion},
	volume = {45},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3155612},
	abstract = {Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization ({DB}) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed {DB} module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion ({ASF}) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed {DB} and {ASF} with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.},
	pages = {919--931},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liao, Minghui and Zou, Zhisheng and Wan, Zhaoyi and Yao, Cong and Bai, Xiang},
	date = {2023-01},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Feature extraction, Fuses, Image segmentation, Inference algorithms, Prediction algorithms, Robustness, Scene text detection, Shape, arbitrary shapes, real-time},
}

@inproceedings{liu_path_2018,
	title = {Path Aggregation Network for Instance Segmentation},
	doi = {10.1109/CVPR.2018.00913},
	abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network ({PANet}) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our {PANet} reach the 1st place in the {COCO} 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. {PANet} is also state-of-the-art on {MVD} and Cityscapes.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {8759--8768},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
	date = {2018-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Feature extraction, Image segmentation, Object detection, Proposals, Semantics, Task analysis, Training},
}

@inproceedings{lin_feature_2017,
	title = {Feature Pyramid Networks for Object Detection},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network ({FPN}), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-{CNN} system, our method achieves state-of-the-art single-model results on the {COCO} detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the {COCO} 2016 challenge winners. In addition, our method can run at 5 {FPS} on a {GPU} and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {936--944},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	date = {2017-07},
	note = {{ISSN}: 1063-6919},
	keywords = {Computer architecture, Detectors, Feature extraction, Object detection, Proposals, Robustness, Semantics},
}

@inproceedings{he_mask_2017,
	title = {Mask R-{CNN}},
	doi = {10.1109/ICCV.2017.322},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-{CNN}, extends Faster R-{CNN} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-{CNN} is simple to train and adds only a small overhead to Faster R-{CNN}, running at 5 fps. Moreover, Mask R-{CNN} is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the {COCO} suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-{CNN} outperforms all existing, single-model entries on every task, including the {COCO} 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2980--2988},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	date = {2017-10},
	note = {{ISSN}: 2380-7504},
	keywords = {Feature extraction, Image segmentation, Object detection, Quantization (signal), Robustness, Semantics},
}

@article{he_spatial_2015,
	title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	volume = {37},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2015.2389824},
	abstract = {Existing deep convolutional neural networks ({CNNs}) require a fixed-size (e.g., 224\${\textbackslash}times\$ 224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called {SPP}-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, {SPP}-net should in general improve all {CNN}-based image classification methods. On the {ImageNet} 2012 dataset, we demonstrate that {SPP}-net boosts the accuracy of a variety of {CNN} architectures despite their different designs. On the Pascal {VOC} 2007 and Caltech101 datasets, {SPP}-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of {SPP}-net is also significant in object detection. Using {SPP}-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 \${\textbackslash}times\$ faster than the R-{CNN} method, while achieving better or comparable accuracy on Pascal {VOC} 2007. In {ImageNet} Large Scale Visual Recognition Challenge ({ILSVRC}) 2014, our methods rank \#2 in object detection and \#3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
	pages = {1904--1916},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2015-09},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Accuracy, Agriculture, Convolutional Neural Networks, Convolutional codes, Convolutional neural networks, Feature extraction, Image Classification, Object Detection, Spatial Pyramid Pooling, Testing, Training, Vectors, image classification, object detection, spatial pyramid pooling},
}

@article{uijlings_selective_2013,
	title = {Selective Search for Object Recognition},
	volume = {104},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/{\textasciitilde}uijlings/{SelectiveSearch}.html).},
	pages = {154--171},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Uijlings, Jasper and Sande, K. and Gevers, T. and Smeulders, A.W.M.},
	date = {2013-09-01},
}

@article{ren_faster_2017,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2577031},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} [1] and Fast R-{CNN} [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The {RPN} is trained end-to-end to generate high-quality region proposals, which are used by Fast R-{CNN} for detection. We further merge {RPN} and Fast R-{CNN} into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the {RPN} component tells the unified network where to look. For the very deep {VGG}-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007, 2012, and {MS} {COCO} datasets with only 300 proposals per image. In {ILSVRC} and {COCO} 2015 competitions, Faster R-{CNN} and {RPN} are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	pages = {1137--1149},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	date = {2017-06},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Convolutional codes, Detectors, Feature extraction, Object detection, Proposals, Search problems, Training, convolutional neural network, region proposal},
}

@inproceedings{girshick_fast_2015,
	title = {Fast R-{CNN}},
	doi = {10.1109/ICCV.2015.169},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-{CNN}) for object detection. Fast R-{CNN} builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-{CNN} employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-{CNN} trains the very deep {VGG}16 network 9x faster than R-{CNN}, is 213x faster at test-time, and achieves a higher {mAP} on {PASCAL} {VOC} 2012. Compared to {SPPnet}, Fast R-{CNN} trains {VGG}16 3x faster, tests 10x faster, and is more accurate. Fast R-{CNN} is implemented in Python and C++ (using Caffe) and is available under the open-source {MIT} License at https://github.com/rbgirshick/fast-rcnn.},
	eventtitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {1440--1448},
	booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {Girshick, Ross},
	date = {2015-12},
	note = {{ISSN}: 2380-7504},
	keywords = {Computer architecture, Feature extraction, Object detection, Open source software, Pipelines, Proposals, Training},
}

@inproceedings{girshick_rich_2014,
	title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
	doi = {10.1109/CVPR.2014.81},
	abstract = {Object detection performance, as measured on the canonical {PASCAL} {VOC} dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision ({mAP}) by more than 30\% relative to the previous best result on {VOC} 2012 – achieving a {mAP} of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks ({CNNs}) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with {CNNs}, we call our method R-{CNN}: Regions with {CNN} features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
	eventtitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {580--587},
	booktitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	date = {2014-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Feature extraction, Object detection, Proposals, Support vector machines, Training, Vectors, Visualization},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2023-01-14},
	date = {2012},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks ({GTN}), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	pages = {2278--2324},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	date = {1998-11},
	note = {Conference Name: Proceedings of the {IEEE}},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
}

@article{felzenszwalb_object_nodate,
	title = {Object Detection with Discriminatively Trained Part Based Models},
	abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the {PASCAL} object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difﬁcult benchmarks such as the {PASCAL} datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a marginsensitive approach for data-mining hard negative examples with a formalism we call latent {SVM}. A latent {SVM} is a reformulation of {MI}-{SVM} in terms of latent variables. A latent {SVM} is semi-convex and the training problem becomes convex once latent information is speciﬁed for the positive examples. This leads to an iterative training algorithm that alternates between ﬁxing latent values for positive examples and optimizing the latent {SVM} objective function.},
	author = {Felzenszwalb, Pedro F and Girshick, Ross B and {McAllester}, David and Ramanan, Deva},
	langid = {english},
}

@inproceedings{dalal_histograms_2005,
	location = {San Diego, {CA}, {USA}},
	title = {Histograms of Oriented Gradients for Human Detection},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467360/},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition, adopting linear {SVM} based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient ({HOG}) descriptors signiﬁcantly outperform existing feature sets for human detection. We study the inﬂuence of each stage of the computation on performance, concluding that ﬁne-scale gradients, ﬁne orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original {MIT} pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	eventtitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	pages = {886--893},
	booktitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	publisher = {{IEEE}},
	author = {Dalal, N. and Triggs, B.},
	urldate = {2023-01-13},
	date = {2005},
	langid = {english},
}

@inproceedings{lowe_object_1999,
	location = {Kerkyra, Greece},
	title = {Object recognition from local scale-invariant features},
	isbn = {978-0-7695-0164-2},
	url = {http://ieeexplore.ieee.org/document/790410/},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and afﬁne or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efﬁciently detected through a staged ﬁltering approach that identiﬁes stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identiﬁes candidate object matches. Final veriﬁcation of each match is achieved by ﬁnding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
	eventtitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	pages = {1150--1157 vol.2},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Lowe, D.G.},
	urldate = {2023-01-13},
	date = {1999},
	langid = {english},
}

@article{wang_object_2021,
	title = {From object detection to text detection and recognition: A brief evolution history of optical character recognition},
	volume = {13},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1547},
	doi = {10.1002/wics.1547},
	shorttitle = {From object detection to text detection and recognition},
	abstract = {Text detection and recognition, which is also known as optical character recognition ({OCR}), is an active research area under quick development with a lot of exciting applications. Deep-learning-based methods represent the state-of-art of this area. However, these methods are largely deterministic: they give a deterministic output for each input. For both statisticians and general users, methods supporting uncertainty inference are of great appeal, leaving rich research opportunities to incorporate statistical models and methods with the established deep-learning-based approaches. In this paper, we provide a comprehensive review of the evolution history of research development on {OCR} with discussions on the statistical insights behind these developments and potential directions to enhance the current methods with statistical approaches. We hope this article can serve as a useful guidebook for statisticians who are seeking for a path toward edge-cutting research in this exciting area. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Deep Learning Data: Types and Structure {\textgreater} Image and Spatial Data},
	pages = {e1547},
	number = {5},
	journaltitle = {{WIREs} Computational Statistics},
	author = {Wang, Haifeng and Pan, Changzai and Guo, Xiao and Ji, Chunlin and Deng, Ke},
	urldate = {2023-01-13},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1547},
	keywords = {{OCR}, deep learning, object detection, text detection, text recognition},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on {AdaBoost}, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	eventtitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	pages = {I--I},
	booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	date = {2001-12},
	note = {{ISSN}: 1063-6919},
	keywords = {Detectors, Face detection, Filters, Focusing, Image representation, Machine learning, Object detection, Pixel, Robustness, Skin, td\_methods},
}

@inproceedings{berchmans_optical_2014,
	title = {Optical character recognition: An overview and an insight},
	doi = {10.1109/ICCICCT.2014.6993174},
	shorttitle = {Optical character recognition},
	abstract = {Many researches are going on in the field of optical character recognition ({OCR}) for the last few decades and a lot of articles have been published. Also a large number of {OCR} is available commercially. In this literature a review of the {OCR} history and the various techniques used for {OCR} development in the chronological order is being done.},
	eventtitle = {2014 International Conference on Control, Instrumentation, Communication and Computational Technologies ({ICCICCT})},
	pages = {1361--1365},
	booktitle = {2014 International Conference on Control, Instrumentation, Communication and Computational Technologies ({ICCICCT})},
	author = {Berchmans, Deepa and Kumar, S S},
	date = {2014-07},
	keywords = {Character recognition, Feature extraction, Handwriting recognition, Hidden Markov models, Image recognition, Optical Character Recognition, Optical character recognition software, document analysis, glyph recognition, handwritten characters, intelligent character recognition, offline recognition, online recognition, pattern recognition, printed characters},
}
