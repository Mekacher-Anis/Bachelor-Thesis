
@article{yujian_normalized_2007,
	title = {A Normalized Levenshtein Distance Metric},
	volume = {29},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2007.1078},
	abstract = {Although a number of normalized edit distances presented so far may offer good performance in some applications, none of them can be regarded as a genuine metric between strings because they do not satisfy the triangle inequality. Given two strings X and Y over a finite alphabet, this paper defines a new normalized edit distance between X and Y as a simple function of their lengths ({\textbar}X{\textbar} and {\textbar}Y{\textbar}) and the Generalized Levenshtein Distance ({GLD}) between them. The new distance can be easily computed through {GLD} with a complexity of O({\textbar}X{\textbar} {\textbackslash}cdot {\textbar}Y{\textbar}) and it is a metric valued in [0, 1] under the condition that the weight function is a metric over the set of elementary edit operations with all costs of insertions/deletions having the same weight. Experiments using the {AESA} algorithm in handwritten digit recognition show that the new distance can generally provide similar results to some other normalized edit distances and may perform slightly better if the triangle inequality is violated in a particular data set.},
	pages = {1091--1095},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Yujian, Li and Bo, Liu},
	date = {2007-06},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {{AESA}., Biomedical signal processing, Computational biology, Cost function, Error correction, Handwriting recognition, Image recognition, Information retrieval, Levenshtein distance, Pattern recognition, Sequence comparison, Sequences, Signal processing algorithms, metric, normalized edit distance},
}

@article{zhu_relational_2022,
	title = {Relational Reasoning Over Spatial-Temporal Graphs for Video Summarization},
	volume = {31},
	issn = {1941-0042},
	doi = {10.1109/TIP.2022.3163855},
	abstract = {In this paper, we propose a dynamic graph modeling approach to learn spatial-temporal representations for video summarization. Most existing video summarization methods extract image-level features with {ImageNet} pre-trained deep models. Differently, our method exploits object-level and relation-level information to capture spatial-temporal dependencies. Specifically, our method builds spatial graphs on the detected object proposals. Then, we construct a temporal graph by using the aggregated representations of spatial graphs. Afterward, we perform relational reasoning over spatial and temporal graphs with graph convolutional networks and extract spatial-temporal representations for importance score prediction and key shot selection. To eliminate relation clutters caused by densely connected nodes, we further design a self-attention edge pooling module, which disregards meaningless relations of graphs. We conduct extensive experiments on two popular benchmarks, including the {SumMe} and {TVSum} datasets. Experimental results demonstrate that the proposed method achieves superior performance against state-of-the-art video summarization methods.},
	pages = {3017--3031},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Zhu, Wencheng and Han, Yucheng and Lu, Jiwen and Zhou, Jie},
	date = {2022},
	note = {Conference Name: {IEEE} Transactions on Image Processing},
	keywords = {Adversarial machine learning, Cognition, Feature extraction, Image edge detection, Proposals, Video sequences, Video summarization, Visualization, graph pooling, relational reasoning, self-attention, spatial-temporal representation},
}

@article{zhu_dsnet_2021,
	title = {{DSNet}: A Flexible Detect-to-Summarize Network for Video Summarization},
	volume = {30},
	issn = {1941-0042},
	doi = {10.1109/TIP.2020.3039886},
	shorttitle = {{DSNet}},
	abstract = {In this paper, we propose a Detect-to-Summarize network ({DSNet}) framework for supervised video summarization. Our {DSNet} contains anchor-based and anchor-free counterparts. The anchor-based method generates temporal interest proposals to determine and localize the representative contents of video sequences, while the anchor-free method eliminates the pre-defined temporal proposals and directly predicts the importance scores and segment locations. Different from existing supervised video summarization methods which formulate video summarization as a regression problem without temporal consistency and integrity constraints, our interest detection framework is the first attempt to leverage temporal consistency via the temporal interest detection formulation. Specifically, in the anchor-based approach, we first provide a dense sampling of temporal interest proposals with multi-scale intervals that accommodate interest variations in length, and then extract their long-range temporal features for interest proposal location regression and importance prediction. Notably, positive and negative segments are both assigned for the correctness and completeness information of the generated summaries. In the anchor-free approach, we alleviate drawbacks of temporal proposals by directly predicting importance scores of video frames and segment locations. Particularly, the interest detection framework can be flexibly plugged into off-the-shelf supervised video summarization methods. We evaluate the anchor-based and anchor-free approaches on the {SumMe} and {TVSum} datasets. Experimental results clearly validate the effectiveness of the anchor-based and anchor-free approaches.},
	pages = {948--962},
	journaltitle = {{IEEE} Transactions on Image Processing},
	author = {Zhu, Wencheng and Lu, Jiwen and Li, Jiahao and Zhou, Jie},
	date = {2021},
	note = {Conference Name: {IEEE} Transactions on Image Processing},
	keywords = {Feature extraction, Proposals, Reinforcement learning, Semantics, Training, Video sequences, Video summarization, Visualization, anchor-based detection, anchor-free detection, interest proposal, temporal modeling},
}

@inproceedings{apostolidis_combining_2021,
	title = {Combining Global and Local Attention with Positional Encoding for Video Summarization},
	doi = {10.1109/ISM52913.2021.00045},
	abstract = {This paper presents a new method for supervised video summarization. To overcome drawbacks of existing {RNN}-based summarization architectures, that relate to the modeling of long-range frames’ dependencies and the ability to parallelize the training process, the developed model re-lies on the use of self-attention mechanisms to estimate the importance of video frames. Contrary to previous attention-based summarization approaches that model the frames’ dependencies by observing the entire frame sequence, our method combines global and local multi-head attention mechanisms to discover different modelings of the frames’ dependencies at different levels of granularity. Moreover, the utilized attention mechanisms integrate a component that encodes the temporal position of video frames - this is of major importance when producing a video summary. Experiments on two datasets ({SumMe} and {TVSum}) demonstrate the effectiveness of the proposed model compared to existing attention-based methods, and its competitiveness against other state-of-the-art supervised summarization approaches. An ablation study that focuses on our main proposed components, namely the use of global and local multi-head attention mechanisms in collaboration with an absolute positional encoding component, shows their relative contributions to the overall summarization performance.},
	eventtitle = {2021 {IEEE} International Symposium on Multimedia ({ISM})},
	pages = {226--234},
	booktitle = {2021 {IEEE} International Symposium on Multimedia ({ISM})},
	author = {Apostolidis, Evlampios and Balaouras, Georgios and Mezaris, Vasileios and Patras, Ioannis},
	date = {2021-11},
	keywords = {Collaboration, Encoding, Focusing, Network architecture, Proposals, Streaming media, Training, multi-head attention, positional encoding, self-attention, supervised learning, video summarization},
}

@online{noauthor_how_2016,
	title = {How many languages are there in the world?},
	url = {https://www.ethnologue.com/guides/how-many-languages},
	abstract = {7,151 languages are spoken today, but just 23 languages account for more than half the world’s population.},
	titleaddon = {Ethnologue},
	urldate = {2023-01-27},
	date = {2016-05-03},
	langid = {english},
}

@misc{dosovitskiy_image_2021,
	title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	shorttitle = {An Image is Worth 16x16 Words},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on {CNNs} is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks ({ImageNet}, {CIFAR}-100, {VTAB}, etc.), Vision Transformer ({ViT}) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	publisher = {{arXiv}},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	urldate = {2023-01-25},
	date = {2021-06-03},
	eprinttype = {arxiv},
	eprint = {2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{bautista_scene_2022,
	title = {Scene Text Recognition with Permuted Autoregressive Sequence Models},
	url = {http://arxiv.org/abs/2207.06966},
	doi = {10.48550/arXiv.2207.06966},
	abstract = {Context-aware {STR} methods typically use internal autoregressive ({AR}) language models ({LM}). Inherent limitations of {AR} models motivated two-stage methods which employ an external {LM}. The conditional independence of the external {LM} on the input image may cause it to erroneously rectify correct predictions, leading to significant inefficiencies. Our method, {PARSeq}, learns an ensemble of internal {AR} {LMs} with shared weights using Permutation Language Modeling. It unifies context-free non-{AR} and context-aware {AR} inference, and iterative refinement using bidirectional context. Using synthetic training data, {PARSeq} achieves state-of-the-art ({SOTA}) results in {STR} benchmarks (91.9\% accuracy) and more challenging datasets. It establishes new {SOTA} results (96.0\% accuracy) when trained on real data. {PARSeq} is optimal on accuracy vs parameter count, {FLOPS}, and latency because of its simple, unified structure and parallel token processing. Due to its extensive use of attention, it is robust on arbitrarily-oriented text which is common in real-world images. Code, pretrained weights, and data are available at: https://github.com/baudm/parseq.},
	publisher = {{arXiv}},
	author = {Bautista, Darwin and Atienza, Rowel},
	urldate = {2023-01-25},
	date = {2022-07-14},
	eprinttype = {arxiv},
	eprint = {2207.06966 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@article{lu_master_2021,
	title = {{MASTER}: Multi-Aspect Non-local Network for Scene Text Recognition},
	volume = {117},
	issn = {00313203},
	url = {http://arxiv.org/abs/1910.02562},
	doi = {10.1016/j.patcog.2021.107980},
	shorttitle = {{MASTER}},
	abstract = {Attention-based scene text recognizers have gained huge success, which leverages a more compact intermediate representation to learn 1d- or 2d- attention by a {RNN}-based encoder-decoder architecture. However, such methods suffer from attention-drift problem because high similarity among encoded features leads to attention confusion under the {RNN}-based local attention mechanism. Moreover, {RNN}-based methods have low efficiency due to poor parallelization. To overcome these problems, we propose the {MASTER}, a self-attention based scene text recognizer that (1) not only encodes the input-output attention but also learns self-attention which encodes feature-feature and target-target relationships inside the encoder and decoder and (2) learns a more powerful and robust intermediate representation to spatial distortion, and (3) owns a great training efficiency because of high training parallelization and a high-speed inference because of an efficient memory-cache mechanism. Extensive experiments on various benchmarks demonstrate the superior performance of our {MASTER} on both regular and irregular scene text. Pytorch code can be found at https://github.com/wenwenyu/{MASTER}-pytorch, and Tensorflow code can be found at https://github.com/jiangxiluning/{MASTER}-{TF}.},
	pages = {107980},
	journaltitle = {Pattern Recognition},
	shortjournal = {Pattern Recognition},
	author = {Lu, Ning and Yu, Wenwen and Qi, Xianbiao and Chen, Yihao and Gong, Ping and Xiao, Rong and Bai, Xiang},
	urldate = {2023-01-23},
	date = {2021-09},
	eprinttype = {arxiv},
	eprint = {1910.02562 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{cao_gcnet_2019,
	title = {{GCNet}: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond},
	doi = {10.1109/ICCVW.2019.00246},
	shorttitle = {{GCNet}},
	abstract = {The Non-Local Network ({NLNet}) presents a pioneering approach for capturing long-range dependencies, via aggregating query-specific global context to each query position. However, through a rigorous empirical analysis, we have found that the global contexts modeled by non-local network are almost the same for different query positions within an image. In this paper, we take advantage of this finding to create a simplified network based on a query-independent formulation, which maintains the accuracy of {NLNet} but with significantly less computation. We further observe that this simplified design shares similar structure with Squeeze-Excitation Network ({SENet}). Hence we unify them into a three-step general framework for global context modeling. Within the general framework, we design a better instantiation, called the global context ({GC}) block, which is lightweight and can effectively model the global context. The lightweight property allows us to apply it for multiple layers in a backbone network to construct a global context network ({GCNet}), which generally outperforms both simplified {NLNet} and {SENet} on major benchmarks for various recognition tasks.},
	eventtitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision Workshop ({ICCVW})},
	pages = {1971--1980},
	booktitle = {2019 {IEEE}/{CVF} International Conference on Computer Vision Workshop ({ICCVW})},
	author = {Cao, Yue and Xu, Jiarui and Lin, Stephen and Wei, Fangyun and Hu, Han},
	date = {2019-10},
	note = {{ISSN}: 2473-9944},
	keywords = {Computational modeling, Computer architecture, Context modeling, Convolution, Object detection, Task analysis, Visualization, network archietcture, nonlocal, senet},
}

@inproceedings{sheng_nrtr_2019,
	title = {{NRTR}: A No-Recurrence Sequence-to-Sequence Model for Scene Text Recognition},
	doi = {10.1109/ICDAR.2019.00130},
	shorttitle = {{NRTR}},
	abstract = {Scene text recognition has attracted a great many researches due to its importance to various applications. Existing methods mainly adopt recurrence or convolution based networks. Though have obtained good performance, these methods still suffer from two limitations: slow training speed due to the internal recurrence of {RNNs}, and high complexity due to stacked convolutional layers for long-term feature extraction. This paper, for the first time, proposes a no-recurrence sequence-to-sequence text recognizer, named {NRTR}, that dispenses with recurrences and convolutions entirely. {NRTR} follows the encoder-decoder paradigm, where the encoder uses stacked self-attention to extract image features, and the decoder applies stacked self-attention to recognize texts based on encoder output. {NRTR} relies solely on self-attention mechanism thus could be trained with more parallelization and less complexity. Considering scene image has large variation in text and background, we further design a modality-transform block to effectively transform 2D input images to 1D sequences, combined with the encoder to extract more discriminative features. {NRTR} achieves state-of-the-art or highly competitive performance on both regular and irregular benchmarks, while requires only a small fraction of training time compared to the best model from the literature (at least 8 times faster).},
	eventtitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	pages = {781--786},
	booktitle = {2019 International Conference on Document Analysis and Recognition ({ICDAR})},
	author = {Sheng, Fenfen and Chen, Zhineng and Xu, Bo},
	date = {2019-09},
	note = {{ISSN}: 2379-2140},
	keywords = {Complexity theory, Convolution, Decoding, Faster and better, Feature extraction, Image recognition, Modality transform block, No-Recurrence, Self-attention, Text recognition, Training},
}

@inproceedings{lee_recognizing_2020,
	title = {On Recognizing Texts of Arbitrary Shapes with 2D Self-Attention},
	doi = {10.1109/CVPRW50498.2020.00281},
	abstract = {Scene text recognition ({STR}) is the task of recognizing character sequences in natural scenes. While there have been great advances in {STR} methods, current methods which convert two-dimensional (2D) image to one-dimensional (1D) feature map still fail to recognize texts in arbitrary shapes, such as heavily curved, rotated or vertically aligned texts, which are abundant in daily life (e.g. restaurant signs, product labels, company logos, etc). This paper introduces an architecture to recognize texts of arbitrary shapes, named Self-Attention Text Recognition Network ({SATRN}). {SATRN} utilizes the self-attention mechanism, which is originally proposed to capture the dependency between word tokens in a sentence, to describe 2D spatial dependencies of characters in a scene text image. Exploiting the full-graph propagation of self-attention, {SATRN} can recognize texts with arbitrary arrangements and large inter-character spacing. As a result, our model outperforms all existing {STR} models by a large margin of 4.5pp on average in "irregular text" benchmarks and also achieved state-of-the-art performance in two "regular text" benchmarks. We provide empirical analyses that illustrate the inner mechanisms and the extent to which the model is applicable (e.g. rotated and multi-line text). We will open- source the code.1},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {2326--2335},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	author = {Lee, Junyeop and Park, Sungrae and Baek, Jeonghun and Oh, Seong Joon and Kim, Seonghyeon and Lee, Hwalsuk},
	date = {2020-06},
	note = {{ISSN}: 2160-7516},
	keywords = {Benchmark testing, Decoding, Feature extraction, Shape, Task analysis, Text recognition, Two dimensional displays},
}

@inproceedings{li_show_2019,
	location = {Honolulu, Hawaii, {USA}},
	title = {Show, attend and read: a simple and strong baseline for irregular text recognition},
	isbn = {978-1-57735-809-1},
	url = {https://doi.org/10.1609/aaai.v33i01.33018610},
	doi = {10.1609/aaai.v33i01.33018610},
	series = {{AAAI}'19/{IAAI}'19/{EAAI}'19},
	shorttitle = {Show, attend and read},
	abstract = {Recognizing irregular text in natural scene images is challenging due to the large variance in text appearance, such as curvature, orientation and distortion. Most existing approaches rely heavily on sophisticated model designs and/or extra fine-grained annotations, which, to some extent, increase the difficulty in algorithm implementation and data collection. In this work, we propose an easy-to-implement strong baseline for irregular scene text recognition, using off-the-shelf neural network components and only word-level annotations. It is composed of a 3f-layer {ResNet}, an {LSTM}-based encoder-decoder framework and a 2-dimensional attention module. Despite its simplicity, the proposed method is robust. It achieves state-of-the-art performance on irregular text recognition benchmarks and comparable results on regular text datasets. The code will be released.},
	pages = {8610--8617},
	booktitle = {Proceedings of the Thirty-Third {AAAI} Conference on Artificial Intelligence and Thirty-First Innovative Applications of Artificial Intelligence Conference and Ninth {AAAI} Symposium on Educational Advances in Artificial Intelligence},
	publisher = {{AAAI} Press},
	author = {Li, Hui and Wang, Peng and Shen, Chunhua and Zhang, Guyu},
	urldate = {2023-01-20},
	date = {2019-01-27},
}

@inproceedings{vaswani_attention_2017,
	location = {Red Hook, {NY}, {USA}},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	series = {{NIPS}'17},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 {BLEU} on the {WMT} 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 {BLEU}. On the {WMT} 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art {BLEU} score of 41.0 after training for 3.5 days on eight {GPUs}, a small fraction of the training costs of the best models from the literature.},
	pages = {6000--6010},
	booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	urldate = {2023-01-20},
	date = {2017-12-04},
}

@article{shi_end--end_2017,
	title = {An End-to-End Trainable Neural Network for Image-Based Sequence Recognition and Its Application to Scene Text Recognition},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2646371},
	abstract = {Image-based sequence recognition has been a long-standing research topic in computer vision. In this paper, we investigate the problem of scene text recognition, which is among the most important and challenging tasks in image-based sequence recognition. A novel neural network architecture, which integrates feature extraction, sequence modeling and transcription into a unified framework, is proposed. Compared with previous systems for scene text recognition, the proposed architecture possesses four distinctive properties: (1) It is end-to-end trainable, in contrast to most of the existing algorithms whose components are separately trained and tuned. (2) It naturally handles sequences in arbitrary lengths, involving no character segmentation or horizontal scale normalization. (3) It is not confined to any predefined lexicon and achieves remarkable performances in both lexicon-free and lexicon-based scene text recognition tasks. (4) It generates an effective yet much smaller model, which is more practical for realworld application scenarios. The experiments on standard benchmarks, including the {IIIT}-5K, Street View Text and {ICDAR} datasets, demonstrate the superiority of the proposed algorithm over the prior arts. Moreover, the proposed algorithm performs well in the task of image-based music score recognition, which evidently verifies the generality of it.},
	pages = {2298--2304},
	number = {11},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Shi, Baoguang and Bai, Xiang and Yao, Cong},
	date = {2017-11},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Context, Convolutional codes, Feature extraction, Image recognition, Logic gates, Neural networks, Sequence recognition, Text recognition, convolutional neural network, long-short term memory, neural network, optical music recognition, scene text recognition},
}

@inproceedings{zhang_deep_2020,
	title = {Deep Relational Reasoning Graph Network for Arbitrary Shape Text Detection},
	doi = {10.1109/CVPR42600.2020.00972},
	abstract = {Arbitrary shape text detection is a challenging task due to the high variety and complexity of scenes texts. In this paper, we propose a novel unified relational reasoning graph network for arbitrary shape text detection. In our method, an innovative local graph bridges a text proposal model via Convolutional Neural Network ({CNN}) and a deep relational reasoning network via Graph Convolutional Network ({GCN}), making our network end-to-end trainable. To be concrete, every text instance will be divided into a series of small rectangular components, and the geometry attributes (e.g., height, width, and orientation) of the small components will be estimated by our text proposal model. Given the geometry attributes, the local graph construction model can roughly establish linkages between different text components. For further reasoning and deducing the likelihood of linkages between the component and its neighbors, we adopt a graph-based network to perform deep relational reasoning on local graphs. Experiments on public available datasets demonstrate the state-of-the-art performance of our method. Code is available at https://github.com/{GXYM}/{DRRG}.},
	eventtitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {9696--9705},
	booktitle = {2020 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Zhang, Shi-Xue and Zhu, Xiaobin and Hou, Jie-Bo and Liu, Chang and Yang, Chun and Wang, Hongfa and Yin, Xu-Cheng},
	date = {2020-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Cognition, Convolution, Couplings, Geometry, Proposals, Shape, Task analysis},
}

@inproceedings{riaz_fouriernet_2021,
	title = {{FourierNet}: Compact Mask Representation for Instance Segmentation Using Differentiable Shape Decoders},
	doi = {10.1109/ICPR48806.2021.9413048},
	shorttitle = {{FourierNet}},
	abstract = {We present {FourierNet}, a single shot, anchor-free, fully convolutional instance segmentation method that predicts a shape vector. Consequently, this shape vector is converted into the masks' contour points using a fast numerical transform. Compared to previous methods, we introduce a new training technique, where we utilize a differentiable shape decoder, which manages the automatic weight balancing of the shape vector's coefficients. We used the Fourier series as a shape encoder because of its coefficient interpretability and fast implementation. {FourierNet} shows promising results compared to polygon representation methods, achieving 30.6 {mAP} on the {MS} {COCO} 2017 benchmark. At lower image resolutions, it runs at 26.6 {FPS} with 24.3 {mAP}. It reaches 23.3 {mAP} using just eight parameters to represent the mask (note that at least four parameters are needed for bounding box prediction only). Qualitative analysis shows that suppressing a reasonable proportion of higher frequencies of Fourier series, still generates meaningful masks. These results validate our understanding that lower frequency components hold higher information for the segmentation task, and therefore, we can achieve a compressed representation. Code is available at: github.com/cogsys-tuebingen/{FourierNet}.},
	eventtitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	pages = {7833--7840},
	booktitle = {2020 25th International Conference on Pattern Recognition ({ICPR})},
	author = {Riaz, Hamd Ul Moqeet and Benbarka, Nuri and Zell, Andreas},
	date = {2021-01},
	note = {{ISSN}: 1051-4651},
	keywords = {Decoding, Detectors, Fourier series, Image resolution, Instance segmentation, Real-time systems, Shape, Training, Transforms, anchor-free, differentiable algorithms, shape encoding},
}

@inproceedings{wang_shape_2019,
	title = {Shape Robust Text Detection With Progressive Scale Expansion Network},
	doi = {10.1109/CVPR.2019.00956},
	abstract = {Scene text detection has witnessed rapid progress especially with the recent development of convolutional neural networks. However, there still exists two challenges which prevent the algorithm into industry applications. On the one hand, most of the state-of-art algorithms require quadrangle bounding box which is in-accurate to locate the texts with arbitrary shape. On the other hand, two text instances which are close to each other may lead to a false detection which covers both instances. Traditionally, the segmentation-based approach can relieve the first problem but usually fail to solve the second challenge. To address these two challenges, in this paper, we propose a novel Progressive Scale Expansion Network ({PSENet}), which can precisely detect text instances with arbitrary shapes. More specifically, {PSENet} generates the different scale of kernels for each text instance, and gradually expands the minimal scale kernel to the text instance with the complete shape. Due to the fact that there are large geometrical margins among the minimal scale kernels, our method is effective to split the close text instances, making it easier to use segmentation-based methods to detect arbitrary-shaped text instances. Extensive experiments on {CTW}1500, Total-Text, {ICDAR} 2015 and {ICDAR} 2017 {MLT} validate the effectiveness of {PSENet}. Notably, on {CTW}1500, a dataset full of long curve texts, {PSENet} achieves a F-measure of 74.3\% at 27 {FPS}, and our best F-measure (82.2\%) outperforms state-of-art algorithms by 6.6\%. The code will be released in the future.},
	eventtitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {9328--9337},
	booktitle = {2019 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Wang, Wenhai and Xie, Enze and Li, Xiang and Hou, Wenbo and Lu, Tong and Yu, Gang and Shao, Shuai},
	date = {2019-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Categorization, Deep Learning, Recognition: Detection, Retrieval},
}

@misc{liao_real-time_2019,
	title = {Real-time Scene Text Detection with Differentiable Binarization},
	url = {http://arxiv.org/abs/1911.08947},
	doi = {10.48550/arXiv.1911.08947},
	abstract = {Recently, segmentation-based methods are quite popular in scene text detection, as the segmentation results can more accurately describe scene text of various shapes such as curve text. However, the post-processing of binarization is essential for segmentation-based detection, which converts probability maps produced by a segmentation method into bounding boxes/regions of text. In this paper, we propose a module named Differentiable Binarization ({DB}), which can perform the binarization process in a segmentation network. Optimized along with a {DB} module, a segmentation network can adaptively set the thresholds for binarization, which not only simplifies the post-processing but also enhances the performance of text detection. Based on a simple segmentation network, we validate the performance improvements of {DB} on five benchmark datasets, which consistently achieves state-of-the-art results, in terms of both detection accuracy and speed. In particular, with a light-weight backbone, the performance improvements by {DB} are significant so that we can look for an ideal tradeoff between detection accuracy and efficiency. Specifically, with a backbone of {ResNet}-18, our detector achieves an F-measure of 82.8, running at 62 {FPS}, on the {MSRA}-{TD}500 dataset. Code is available at: https://github.com/{MhLiao}/{DB}},
	publisher = {{arXiv}},
	author = {Liao, Minghui and Wan, Zhaoyi and Yao, Cong and Chen, Kai and Bai, Xiang},
	urldate = {2023-01-17},
	date = {2019-12-03},
	eprinttype = {arxiv},
	eprint = {1911.08947 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{liao_real-time_2023,
	title = {Real-Time Scene Text Detection With Differentiable Binarization and Adaptive Scale Fusion},
	volume = {45},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2022.3155612},
	abstract = {Recently, segmentation-based scene text detection methods have drawn extensive attention in the scene text detection field, because of their superiority in detecting the text instances of arbitrary shapes and extreme aspect ratios, profiting from the pixel-level descriptions. However, the vast majority of the existing segmentation-based approaches are limited to their complex post-processing algorithms and the scale robustness of their segmentation models, where the post-processing algorithms are not only isolated to the model optimization but also time-consuming and the scale robustness is usually strengthened by fusing multi-scale feature maps directly. In this paper, we propose a Differentiable Binarization ({DB}) module that integrates the binarization process, one of the most important steps in the post-processing procedure, into a segmentation network. Optimized along with the proposed {DB} module, the segmentation network can produce more accurate results, which enhances the accuracy of text detection with a simple pipeline. Furthermore, an efficient Adaptive Scale Fusion ({ASF}) module is proposed to improve the scale robustness by fusing features of different scales adaptively. By incorporating the proposed {DB} and {ASF} with the segmentation network, our proposed scene text detector consistently achieves state-of-the-art results, in terms of both detection accuracy and speed, on five standard benchmarks.},
	pages = {919--931},
	number = {1},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Liao, Minghui and Zou, Zhisheng and Wan, Zhaoyi and Yao, Cong and Bai, Xiang},
	date = {2023-01},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Feature extraction, Fuses, Image segmentation, Inference algorithms, Prediction algorithms, Robustness, Scene text detection, Shape, arbitrary shapes, real-time},
}

@inproceedings{liu_path_2018,
	title = {Path Aggregation Network for Instance Segmentation},
	doi = {10.1109/CVPR.2018.00913},
	abstract = {The way that information propagates in neural networks is of great importance. In this paper, we propose Path Aggregation Network ({PANet}) aiming at boosting information flow in proposal-based instance segmentation framework. Specifically, we enhance the entire feature hierarchy with accurate localization signals in lower layers by bottom-up path augmentation, which shortens the information path between lower layers and topmost feature. We present adaptive feature pooling, which links feature grid and all feature levels to make useful information in each level propagate directly to following proposal subnetworks. A complementary branch capturing different views for each proposal is created to further improve mask prediction. These improvements are simple to implement, with subtle extra computational overhead. Yet they are useful and make our {PANet} reach the 1st place in the {COCO} 2017 Challenge Instance Segmentation task and the 2nd place in Object Detection task without large-batch training. {PANet} is also state-of-the-art on {MVD} and Cityscapes.},
	eventtitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	pages = {8759--8768},
	booktitle = {2018 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition},
	author = {Liu, Shu and Qi, Lu and Qin, Haifang and Shi, Jianping and Jia, Jiaya},
	date = {2018-06},
	note = {{ISSN}: 2575-7075},
	keywords = {Feature extraction, Image segmentation, Object detection, Proposals, Semantics, Task analysis, Training},
}

@inproceedings{lin_feature_2017,
	title = {Feature Pyramid Networks for Object Detection},
	doi = {10.1109/CVPR.2017.106},
	abstract = {Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But pyramid representations have been avoided in recent object detectors that are based on deep convolutional networks, partially because they are slow to compute and memory intensive. In this paper, we exploit the inherent multi-scale, pyramidal hierarchy of deep convolutional networks to construct feature pyramids with marginal extra cost. A top-down architecture with lateral connections is developed for building high-level semantic feature maps at all scales. This architecture, called a Feature Pyramid Network ({FPN}), shows significant improvement as a generic feature extractor in several applications. Using a basic Faster R-{CNN} system, our method achieves state-of-the-art single-model results on the {COCO} detection benchmark without bells and whistles, surpassing all existing single-model entries including those from the {COCO} 2016 challenge winners. In addition, our method can run at 5 {FPS} on a {GPU} and thus is a practical and accurate solution to multi-scale object detection. Code will be made publicly available.},
	eventtitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	pages = {936--944},
	booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})},
	author = {Lin, Tsung-Yi and Dollár, Piotr and Girshick, Ross and He, Kaiming and Hariharan, Bharath and Belongie, Serge},
	date = {2017-07},
	note = {{ISSN}: 1063-6919},
	keywords = {Computer architecture, Detectors, Feature extraction, Object detection, Proposals, Robustness, Semantics},
}

@inproceedings{he_mask_2017,
	title = {Mask R-{CNN}},
	doi = {10.1109/ICCV.2017.322},
	abstract = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-{CNN}, extends Faster R-{CNN} by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-{CNN} is simple to train and adds only a small overhead to Faster R-{CNN}, running at 5 fps. Moreover, Mask R-{CNN} is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the {COCO} suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-{CNN} outperforms all existing, single-model entries on every task, including the {COCO} 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
	eventtitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {2980--2988},
	booktitle = {2017 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {He, Kaiming and Gkioxari, Georgia and Dollár, Piotr and Girshick, Ross},
	date = {2017-10},
	note = {{ISSN}: 2380-7504},
	keywords = {Feature extraction, Image segmentation, Object detection, Quantization (signal), Robustness, Semantics},
}

@article{he_spatial_2015,
	title = {Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition},
	volume = {37},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2015.2389824},
	abstract = {Existing deep convolutional neural networks ({CNNs}) require a fixed-size (e.g., 224\${\textbackslash}times\$ 224) input image. This requirement is “artificial” and may reduce the recognition accuracy for the images or sub-images of an arbitrary size/scale. In this work, we equip the networks with another pooling strategy, “spatial pyramid pooling”, to eliminate the above requirement. The new network structure, called {SPP}-net, can generate a fixed-length representation regardless of image size/scale. Pyramid pooling is also robust to object deformations. With these advantages, {SPP}-net should in general improve all {CNN}-based image classification methods. On the {ImageNet} 2012 dataset, we demonstrate that {SPP}-net boosts the accuracy of a variety of {CNN} architectures despite their different designs. On the Pascal {VOC} 2007 and Caltech101 datasets, {SPP}-net achieves state-of-the-art classification results using a single full-image representation and no fine-tuning. The power of {SPP}-net is also significant in object detection. Using {SPP}-net, we compute the feature maps from the entire image only once, and then pool features in arbitrary regions (sub-images) to generate fixed-length representations for training the detectors. This method avoids repeatedly computing the convolutional features. In processing test images, our method is 24-102 \${\textbackslash}times\$ faster than the R-{CNN} method, while achieving better or comparable accuracy on Pascal {VOC} 2007. In {ImageNet} Large Scale Visual Recognition Challenge ({ILSVRC}) 2014, our methods rank \#2 in object detection and \#3 in image classification among all 38 teams. This manuscript also introduces the improvement made for this competition.},
	pages = {1904--1916},
	number = {9},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	date = {2015-09},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Accuracy, Agriculture, Convolutional Neural Networks, Convolutional codes, Convolutional neural networks, Feature extraction, Image Classification, Object Detection, Spatial Pyramid Pooling, Testing, Training, Vectors, image classification, object detection, spatial pyramid pooling},
}

@article{uijlings_selective_2013,
	title = {Selective Search for Object Recognition},
	volume = {104},
	doi = {10.1007/s11263-013-0620-5},
	abstract = {This paper addresses the problem of generating possible object locations for use in object recognition. We introduce selective search which combines the strength of both an exhaustive search and segmentation. Like segmentation, we use the image structure to guide our sampling process. Like exhaustive search, we aim to capture all possible object locations. Instead of a single technique to generate possible object locations, we diversify our search and use a variety of complementary image partitionings to deal with as many image conditions as possible. Our selective search results in a small set of data-driven, class-independent, high quality locations, yielding 99 \% recall and a Mean Average Best Overlap of 0.879 at 10,097 locations. The reduced number of locations compared to an exhaustive search enables the use of stronger machine learning techniques and stronger appearance models for object recognition. In this paper we show that our selective search enables the use of the powerful Bag-of-Words model for recognition. The selective search software is made publicly available (Software: http://disi.unitn.it/{\textasciitilde}uijlings/{SelectiveSearch}.html).},
	pages = {154--171},
	journaltitle = {International Journal of Computer Vision},
	shortjournal = {International Journal of Computer Vision},
	author = {Uijlings, Jasper and Sande, K. and Gevers, T. and Smeulders, A.W.M.},
	date = {2013-09-01},
}

@article{ren_faster_2017,
	title = {Faster R-{CNN}: Towards Real-Time Object Detection with Region Proposal Networks},
	volume = {39},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2016.2577031},
	shorttitle = {Faster R-{CNN}},
	abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like {SPPnet} [1] and Fast R-{CNN} [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network({RPN}) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An {RPN} is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The {RPN} is trained end-to-end to generate high-quality region proposals, which are used by Fast R-{CNN} for detection. We further merge {RPN} and Fast R-{CNN} into a single network by sharing their convolutional features-using the recently popular terminology of neural networks with 'attention' mechanisms, the {RPN} component tells the unified network where to look. For the very deep {VGG}-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a {GPU}, while achieving state-of-the-art object detection accuracy on {PASCAL} {VOC} 2007, 2012, and {MS} {COCO} datasets with only 300 proposals per image. In {ILSVRC} and {COCO} 2015 competitions, Faster R-{CNN} and {RPN} are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
	pages = {1137--1149},
	number = {6},
	journaltitle = {{IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
	date = {2017-06},
	note = {Conference Name: {IEEE} Transactions on Pattern Analysis and Machine Intelligence},
	keywords = {Convolutional codes, Detectors, Feature extraction, Object detection, Proposals, Search problems, Training, convolutional neural network, region proposal},
}

@inproceedings{girshick_fast_2015,
	title = {Fast R-{CNN}},
	doi = {10.1109/ICCV.2015.169},
	abstract = {This paper proposes a Fast Region-based Convolutional Network method (Fast R-{CNN}) for object detection. Fast R-{CNN} builds on previous work to efficiently classify object proposals using deep convolutional networks. Compared to previous work, Fast R-{CNN} employs several innovations to improve training and testing speed while also increasing detection accuracy. Fast R-{CNN} trains the very deep {VGG}16 network 9x faster than R-{CNN}, is 213x faster at test-time, and achieves a higher {mAP} on {PASCAL} {VOC} 2012. Compared to {SPPnet}, Fast R-{CNN} trains {VGG}16 3x faster, tests 10x faster, and is more accurate. Fast R-{CNN} is implemented in Python and C++ (using Caffe) and is available under the open-source {MIT} License at https://github.com/rbgirshick/fast-rcnn.},
	eventtitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	pages = {1440--1448},
	booktitle = {2015 {IEEE} International Conference on Computer Vision ({ICCV})},
	author = {Girshick, Ross},
	date = {2015-12},
	note = {{ISSN}: 2380-7504},
	keywords = {Computer architecture, Feature extraction, Object detection, Open source software, Pipelines, Proposals, Training},
}

@inproceedings{girshick_rich_2014,
	title = {Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation},
	doi = {10.1109/CVPR.2014.81},
	abstract = {Object detection performance, as measured on the canonical {PASCAL} {VOC} dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision ({mAP}) by more than 30\% relative to the previous best result on {VOC} 2012 – achieving a {mAP} of 53.3\%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks ({CNNs}) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with {CNNs}, we call our method R-{CNN}: Regions with {CNN} features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/ rbg/rcnn.},
	eventtitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition},
	pages = {580--587},
	booktitle = {2014 {IEEE} Conference on Computer Vision and Pattern Recognition},
	author = {Girshick, Ross and Donahue, Jeff and Darrell, Trevor and Malik, Jitendra},
	date = {2014-06},
	note = {{ISSN}: 1063-6919},
	keywords = {Feature extraction, Object detection, Proposals, Support vector machines, Training, Vectors, Visualization},
}

@inproceedings{krizhevsky_imagenet_2012,
	title = {{ImageNet} Classification with Deep Convolutional Neural Networks},
	volume = {25},
	url = {https://proceedings.neurips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.3 million high-resolution images in the {LSVRC}-2010 {ImageNet} training set into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 39.7{\textbackslash}\% and 18.9{\textbackslash}\% which is considerably better than the previous state-of-the-art results. The neural network, which has 60 million parameters and 500,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and two globally connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient {GPU} implementation of convolutional nets. To reduce overfitting in the globally connected layers we employed a new regularization method that proved to be very effective.},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	urldate = {2023-01-14},
	date = {2012},
}

@article{lecun_gradient-based_1998,
	title = {Gradient-based learning applied to document recognition},
	volume = {86},
	issn = {1558-2256},
	doi = {10.1109/5.726791},
	abstract = {Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks ({GTN}), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day.},
	pages = {2278--2324},
	number = {11},
	journaltitle = {Proceedings of the {IEEE}},
	author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
	date = {1998-11},
	note = {Conference Name: Proceedings of the {IEEE}},
	keywords = {Character recognition, Feature extraction, Hidden Markov models, Machine learning, Multi-layer neural network, Neural networks, Optical character recognition software, Optical computing, Pattern recognition, Principal component analysis},
}

@article{felzenszwalb_object_nodate,
	title = {Object Detection with Discriminatively Trained Part Based Models},
	abstract = {We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the {PASCAL} object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difﬁcult benchmarks such as the {PASCAL} datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a marginsensitive approach for data-mining hard negative examples with a formalism we call latent {SVM}. A latent {SVM} is a reformulation of {MI}-{SVM} in terms of latent variables. A latent {SVM} is semi-convex and the training problem becomes convex once latent information is speciﬁed for the positive examples. This leads to an iterative training algorithm that alternates between ﬁxing latent values for positive examples and optimizing the latent {SVM} objective function.},
	author = {Felzenszwalb, Pedro F and Girshick, Ross B and {McAllester}, David and Ramanan, Deva},
	langid = {english},
}

@inproceedings{dalal_histograms_2005,
	location = {San Diego, {CA}, {USA}},
	title = {Histograms of Oriented Gradients for Human Detection},
	volume = {1},
	isbn = {978-0-7695-2372-9},
	url = {http://ieeexplore.ieee.org/document/1467360/},
	doi = {10.1109/CVPR.2005.177},
	abstract = {We study the question of feature sets for robust visual object recognition, adopting linear {SVM} based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient ({HOG}) descriptors signiﬁcantly outperform existing feature sets for human detection. We study the inﬂuence of each stage of the computation on performance, concluding that ﬁne-scale gradients, ﬁne orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original {MIT} pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
	eventtitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	pages = {886--893},
	booktitle = {2005 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition ({CVPR}'05)},
	publisher = {{IEEE}},
	author = {Dalal, N. and Triggs, B.},
	urldate = {2023-01-13},
	date = {2005},
	langid = {english},
}

@inproceedings{lowe_object_1999,
	location = {Kerkyra, Greece},
	title = {Object recognition from local scale-invariant features},
	isbn = {978-0-7695-0164-2},
	url = {http://ieeexplore.ieee.org/document/790410/},
	doi = {10.1109/ICCV.1999.790410},
	abstract = {An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and afﬁne or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efﬁciently detected through a staged ﬁltering approach that identiﬁes stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identiﬁes candidate object matches. Final veriﬁcation of each match is achieved by ﬁnding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.},
	eventtitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	pages = {1150--1157 vol.2},
	booktitle = {Proceedings of the Seventh {IEEE} International Conference on Computer Vision},
	publisher = {{IEEE}},
	author = {Lowe, D.G.},
	urldate = {2023-01-13},
	date = {1999},
	langid = {english},
}

@article{wang_object_2021,
	title = {From object detection to text detection and recognition: A brief evolution history of optical character recognition},
	volume = {13},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1547},
	doi = {10.1002/wics.1547},
	shorttitle = {From object detection to text detection and recognition},
	abstract = {Text detection and recognition, which is also known as optical character recognition ({OCR}), is an active research area under quick development with a lot of exciting applications. Deep-learning-based methods represent the state-of-art of this area. However, these methods are largely deterministic: they give a deterministic output for each input. For both statisticians and general users, methods supporting uncertainty inference are of great appeal, leaving rich research opportunities to incorporate statistical models and methods with the established deep-learning-based approaches. In this paper, we provide a comprehensive review of the evolution history of research development on {OCR} with discussions on the statistical insights behind these developments and potential directions to enhance the current methods with statistical approaches. We hope this article can serve as a useful guidebook for statisticians who are seeking for a path toward edge-cutting research in this exciting area. This article is categorized under: Statistical Learning and Exploratory Methods of the Data Sciences {\textgreater} Deep Learning Data: Types and Structure {\textgreater} Image and Spatial Data},
	pages = {e1547},
	number = {5},
	journaltitle = {{WIREs} Computational Statistics},
	author = {Wang, Haifeng and Pan, Changzai and Guo, Xiao and Ji, Chunlin and Deng, Ke},
	urldate = {2023-01-13},
	date = {2021},
	langid = {english},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1547},
	keywords = {{OCR}, deep learning, object detection, text detection, text recognition},
}

@inproceedings{viola_rapid_2001,
	title = {Rapid object detection using a boosted cascade of simple features},
	volume = {1},
	doi = {10.1109/CVPR.2001.990517},
	abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on {AdaBoost}, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
	eventtitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	pages = {I--I},
	booktitle = {Proceedings of the 2001 {IEEE} Computer Society Conference on Computer Vision and Pattern Recognition. {CVPR} 2001},
	author = {Viola, P. and Jones, M.},
	date = {2001-12},
	note = {{ISSN}: 1063-6919},
	keywords = {Detectors, Face detection, Filters, Focusing, Image representation, Machine learning, Object detection, Pixel, Robustness, Skin, td\_methods},
}

@inproceedings{berchmans_optical_2014,
	title = {Optical character recognition: An overview and an insight},
	doi = {10.1109/ICCICCT.2014.6993174},
	shorttitle = {Optical character recognition},
	abstract = {Many researches are going on in the field of optical character recognition ({OCR}) for the last few decades and a lot of articles have been published. Also a large number of {OCR} is available commercially. In this literature a review of the {OCR} history and the various techniques used for {OCR} development in the chronological order is being done.},
	eventtitle = {2014 International Conference on Control, Instrumentation, Communication and Computational Technologies ({ICCICCT})},
	pages = {1361--1365},
	booktitle = {2014 International Conference on Control, Instrumentation, Communication and Computational Technologies ({ICCICCT})},
	author = {Berchmans, Deepa and Kumar, S S},
	date = {2014-07},
	keywords = {Character recognition, Feature extraction, Handwriting recognition, Hidden Markov models, Image recognition, Optical Character Recognition, Optical character recognition software, document analysis, glyph recognition, handwritten characters, intelligent character recognition, offline recognition, online recognition, pattern recognition, printed characters},
}
