\chapter{Conclusion}

Overall, the proposed OCR pipeline solves the text localization and recognition problem with a good tradeoff between speed and accuracy. The frames selected by the unique frame detection algorithm can be fed as a batch to the YOLOv7 model, and the output of that is then passed to PARSeq for recognition. Then optionally, one can run the dictionary-based error correction, although, as already shown, it might not improve performance that much. As the final step and depending on the use case, the text boxes can be stitched together into longer lines and paragraphs. This doesn't mean that certain stages in the pipeline can't be improved and further developed.

One of these stages is unique frame detection; although it is fast and works quite well for the recordings of presentations and handwritten text on paper, it fails to reduce the number of processed frames in typical live lectures where there are a lot of changes related to the camera or instructor movement. That isn't necessarily a big problem because the subsampling already drastically decreases the number of frames, and the text localization algorithms like YOLOv7 are getting faster. Still, if it would also work for typical lectures, it would serve as a great building block for a lecture video summarization tool.

In addition, the current pipeline only does text localization and recognition, but in real-world lectures, the content on the board typically also includes mathematical expressions, scientific drawings, and tables, especially in STEM fields. In educational videos, like in mathematics, the content may only consist of a mathematical expression, making the proposed pipeline not much of great help. In such cases, transcribing formulas into a searchable format like LATEX may prove to be of great help in content retrieval systems.

In the final stages of the pipeline, after the bounding box stitching, it may also prove helpful to run a context-based error correction algorithm that understands the grammatical context of the sentence and gives different suggestions on how to fix it. This might also be done using a masked-prediction language model by masking OOV words and asking the model to predict them either in a single shot or using an iterative approach.