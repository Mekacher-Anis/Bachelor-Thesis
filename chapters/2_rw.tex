\chapter{Related Work}
\label{ch:rw}

Since the explosion of deep learning methods with the publication of Alexnet in 2013, there have been numerous developments in text/object detection and text recognition techniques.

Until 2017 object detection was mainly based on region proposals. Region proposal methods, as the name suggests, propose a region of the input image to be scanned for the existence of a possible object. This technique was used by \gls{rcnn} \cite{girshick_rich_2014} (2014) and improved upon by Fast \gls{rcnn} \cite{girshick_fast_2015} (2015) and Faster \gls{rcnn} \cite{ren_faster_2017} (2016) with the introduction of Region Proposal Networks. Building on the improvements of Faster R-CNN, Mask R-CNN \cite{he_mask_2017} (2017) also detects objects in the image but based on pixel-wise masks instead of bounding boxes. \gls{panet} \cite{liu_path_2018} (2018) and \gls{psenet} \cite{wang_shape_2019} (2019) are two text detection models that improved on mask R-CNN using FPNs and extra post-processing algorithms. \gls{dbnet} \cite{liao_real-time_2019} (2019) is also very similar in architecture to PSENet but uses a dynamic binarization method when creating the binary masks for segmentation. This was further enhanced in \gls{dbnetpp} \cite{liao_real-time_2023} (2022) by adding an attention mechanism to merge of different scale feature maps from the Feature Pyramid Network. FourierNet \cite{riaz_fouriernet_2021} (2021) took another approach when representing the detection boxes and instead of using bounding box coordinates it uses a more compact Fourier series representation. All these methods were too slow because the same computations were applied multiple times to different proposed regions, making them inefficient. The \gls{yolo} method introduced in 2015 \cite{redmon_you_2016} solved these limitations by running the expensive convolutional layer computation only once and aggregating the results at the end. This model was then further developed into YOLOv2 (2016) \cite{redmon_yolo9000_2017}, YOLOv3 (2018) \cite{redmon_yolov3_2018}, YOLOv4 (2020) \cite{wang_scaled-yolov4_2021}, and YOLOv7 (2022) \cite{wang_yolov7_2022} with each newer version increasing the speed and accuracy of the model.

Before new deep-learning algorithms took off in 2013, traditional text recognition methods like tesseract \cite{smith_overview_2007} used different shape embedding techniques and classifiers like \gls{svm}s to recognize different characters in an image. But since 2013 most text recognition methods made use of deep-learning techniques like \gls{lstm} and \gls{cnn}s; \gls{sar} \cite{li_show_2019} (2019) is one of those models that used \gls{crnn} to recognize texts of arbitrary shapes. Although LSTMs are versatile they have a performance bottleneck as they can only process data sequentially. This problem has been remediated by the introduction of Transformers \cite{vaswani_attention_2017} in 2017 and since then most models were based on this architecture. \gls{nrtr} \cite{sheng_nrtr_2019} (2019) is one such model that combined transformer architecture and CNNs for feature embedding to recognize text efficiently. This was further improved by \gls{satrn} \cite{lee_recognizing_2020} (2020) to make the recognition more robust for arbitrarily shaped text.  \gls{master} \cite{lu_master_2021} (2021) replaced the encoder of the transformer with Global Context Network that spread the attention over the whole image not only on the local features. \gls{abinet} \cite{fang_read_2021} (2021) added an external language model to iteratively rectify incorrectly transcribed words by the transformer. The idea of enhancing the vision models with language models was taken a step further by \gls{parseq} \cite{bautista_scene_2022} (2022) by integrating the language model into the transformer decoder using a multi-modal learning approach.